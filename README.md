# Labo 04 ‚Äì Optimisation, Caching, Load Balancing, Test de charge, Observabilit√©

<img src="https://upload.wikimedia.org/wikipedia/commons/2/2a/Ets_quebec_logo.png" width="250">    
√âTS - LOG430 - Architecture logicielle - Charg√© de laboratoire: Gabriel C. Ullmann.

## üéØ Objectifs d'apprentissage
- Apprendre √† configurer [Prometheus](https://prometheus.io/docs/introduction/overview/)
- Apprendre √† effectuer des tests de charge avec [Locust](https://docs.locust.io/en/stable/what-is-locust.html)
- Comprendre les types d'optimisation possibles dans le contexte du Store Manager ainsi que les avantages et inconv√©nients de chacun
- Apprendre √† impl√©menter le cache en m√©moire avec Redis et l'√©quilibrage de charge (load balancing) avec [Nginx](https://nginx.org/en/docs/http/load_balancing.html)

## ‚öôÔ∏è Setup

Dans ce laboratoire, on continuera √† utiliser la m√™me version du Store Manager d√©velopp√©e au laboratoire 03, mais nous ferons quelques petites modifications. Le but n'est pas d'ajouter de nouvelles fonctionnalit√©s, mais de mesurer et comparer la performance de lecture/√©criture de l'application en utilisant MySQL et Redis. Apr√®s avoir mesur√© et compar√©, nous allons impl√©menter 3 approches d'optimisation : √©liminer le probl√®me N+1, impl√©menter le caching et le load balancing.

En r√©sum√©, dans ce laboratoire, nous nous concentrerons non seulement sur la surveillance (mesure des variables et observation passive), mais aussi sur l'observabilit√© (agir sur nos observations pour modifier le logiciel ou son environnement).

> ‚ö†Ô∏è **IMPORTANT** : Les documents ARC42 et ADR contenus dans ce d√©p√¥t sont identiques √† ceux du laboratoire 03, car nous ne modifions pas l'architecture de l'application dans ce laboratoire.

> üìù **NOTE** : √Ä partir de ce laboratoire, nous vous encourageons √† utiliser la biblioth√®que `logger` plut√¥t que la commande `print`. Bien que `print` fonctionne bien pour le d√©bogage, l'utilisation d'un logger est une bonne pratique de d√©veloppement logiciel car elle offre [plusieurs avantages lorsque notre application entre en production](https://www.geeksforgeeks.org/python/difference-between-logging-and-print-in-python/). Vous trouverez un exemple d'utilisation du `logger` et plus de d√©tails dans `src/stocks/commands/write_stock.py`.

### 1. Cr√©ez un nouveau d√©p√¥t √† partir du gabarit et clonez le d√©p√¥t
```bash
git clone https://github.com/[votredepot]/log430-labo4
cd log430-labo4
```

### 2. Cr√©ez un r√©seau Docker
Ex√©cutez dans votre terminal :
```bash
docker network create labo04-network
```

### 3. G√©n√©rez des donn√©es fictives (mock data)
Pendant ce laboratoire, nous r√©aliserons des **tests de charge** pour √©valuer les limitations de performance de l'application sous une forte pression. Pour simuler un environnement de production r√©aliste, nous utiliserons un volume important de donn√©es g√©n√©r√©es par le script `generators/data_generator.py`. Ex√©cutez ce script sur votre ordinateur **avant de d√©marrer le conteneur Docker**. Il g√©n√©rera automatiquement des commandes INSERT (MySQL) et SET (Redis) pour :

- 1000 utilisateurs
- 10 000 articles (avec quantit√©s de stock al√©atoires)
- 80 000 commandes (contenant 1-5 articles chacune, en utilisant des `product_ids` et `user_ids` al√©atoires)

> üìù **NOTE** : Ces chiffres correspondent aux donn√©es que 2 magasins pourraient accumuler en 1 an d'utilisation continue (‚âà110 commandes/jour), ou 3 magasins pendant 1 an (‚âà75 commandes/jour).

Les commandes MySQL et Redis g√©n√©r√©es seront ex√©cut√©es automatiquement au d√©marrage des conteneurs Docker (√©tape 4). Aucune action suppl√©mentaire n'est requise, mais l'initialisation peut prendre quelques secondes. Vous pouvez surveiller la progression en consultant les logs Docker de vos serveurs MySQL ou Redis.

### 4. Pr√©parez l'environnement de d√©veloppement
Suivez les m√™mes √©tapes que dans le laboratoire dernier (ex. cr√©ation d'un fichier `.env`, etc.).

### 5. Installez Postman
Suivez les m√™mes √©tapes que dans le laboratoire dernier. Importez la collection disponible dans `/docs/collections`.

## üß™ Activit√©s pratiques
Pendant le labo 02, nous avons impl√©ment√© le cache avec Redis. Pendant le labo 03, nous avons utilis√© ce cache pour les endpoints des rapports. Dans ce labo, nous allons temporairement d√©sactiver Redis pour mesurer la diff√©rence entre les lectures √† MySQL vs Redis. Pour faciliter les comparaisons, dans ce laboratoire les m√©thodes qui font la g√©n√©ration de rapports dans `queries/read_order.py` ont 2 versions : une pour MySQL, une autre pour Redis.

### 1. D√©sactivez le cache Redis temporairement
Dans `queries/read_order.py`, remplacez l'appel √† `get_highest_spending_users_redis` par `get_highest_spending_users_mysql`. √âgalement, remplacez l'appel √† `get_best_selling_products_redis` par `get_best_selling_products_mysql`. √áa sera important √† partir de l'activit√© 5.

### 2. Instrumentez Flask avec Prometheus
Avant le test de charge proprement dit, pr√©parons notre code pour l'observabilit√©. Dans `store_manager.py`, ajoutez un endpoint `/metrics`, qui permettra √† Prometheus de lire l'√©tat des variables que nous voulons observer dans l'application.
```python
@app.route("/metrics")
def metrics():
    return generate_latest(), 200, {"Content-Type": CONTENT_TYPE_LATEST}
```

N'oubliez pas d'ajouter √©galement les `imports` suivants :
```python
from prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST
```

### 3. Cr√©ez des Counters 
√âgalement dans `store_manager.py`, ajoutez les objets [Counter](https://prometheus.io/docs/concepts/metric_types/#counter) pour compter le nombre de requ√™tes aux endpoints `/orders`, `/orders/reports/highest-spenders` et `/orders/reports/best-sellers`. N'oubliez pas d'appeler la m√©thode `inc()` pour incr√©menter la valeur du compteur √† chaque requ√™te. Par exemple :

```python
counter_orders = Counter('orders', 'Total calls to /orders')
@app.post('/orders')
def post_orders():
    counter_orders.inc()
```

Red√©marrez les conteneurs Docker.
```bash
docker compose restart store_manager
docker compose restart prometheus              
```

### 4. Observez les m√©triques dans Prometheus
Dans Postman, faites quelques requ√™tes √† `POST /orders`. Ensuite, acc√©dez √† Prometheus sur `http://localhost:9090` et ex√©cutez une requ√™te (query) √† `orders_total`. Vous devriez voir une valeur num√©rique associ√©e √† la variable. Faites la m√™me chose pour les deux autres `Counters`. Par exemple, si vous avez nomm√© le compteur `highest_spenders`, ex√©cutez une requ√™te √† `highest_spenders_total`. Cliquez sur `Graph` pour voir la repr√©sentation visuelle de chaque variable. Faites quelques requ√™tes de plus pour voir le changement des variables.

> üìù **NOTE 1** : Prometheus ne met pas automatiquement √† jour les variables dans l'interface Web lorsqu'elles changent sur le serveur. Vous devez cliquer sur `Query` ou recharger la page Web pour voir les valeurs mises √† jour.

> üìù **NOTE 2** : N'oubliez pas que la surveillance et l'observabilit√© ne concernent pas uniquement les d√©veloppeurs. Dans un environnement professionnel, vous pouvez utiliser des outils tels que [Grafana](https://grafana.com/docs/grafana/latest/setup-grafana/installation/docker/) pour cr√©er des graphiques plus intuitifs et faciles √† utiliser qui peuvent √™tre utilis√©s pour les autres membres de votre √©quipe pour rester inform√©s sur l'√©tat de l'application et pour prendre des d√©cisions. 

### 5. Lancez un test de charge avec Locust
Nos tests de charge seront effectu√©s √† l'aide de Locust, un outil web qui ex√©cute les tests d√©finis par nous dans `locustfiles`. Le script `locustfiles/locustfile.py` lorsqu'il est ex√©cut√©, effectuera plusieurs appels vers des endpoints (repr√©sent√©s par les m√©thodes `@task`), simulant des utilisateurs r√©els. Il appelera :
- L'endpoint `POST /orders` pour tester l'√©criture. Le script cr√©era des commandes en utilisant des articles, des quantit√©s et des utilisateurs al√©atoires.
- Les endpoints `GET /orders/reports/highest-spenders` et `GET /orders/reports/best-sellers` pour tester la lecture. 

Dans ce labo, nous ne modifierons pas le `locustfile`, nous l'activerons simplement √† partir de l'interface web de Locust. Si vous √™tes curieux d'en savoir plus sur comment √©crire des scripts de test de charge plus complexes, vous pouvez trouver plus d'informations sur la [documentation officielle de Locust](https://docs.locust.io/en/stable/writing-a-locustfile.html).

Pour ex√©cuter le test, acc√©dez √† `http://localhost:8089` et appliquez les param√®tres suivants :
- **Number of users (nombre total d'utilisateurs)** : 150
- **Spawn rate (taux d'apparition des nouveaux utilisateurs)** : 2 (par seconde)
- **Host** : Il est pr√©f√©rable d'ex√©cuter les tests de charge sur un serveur externe (par exemple, une VM LXD). Ouvrez le port 5000 dans la VM et d'autres ports si n√©cessaire. Si vous n'avez pas acc√®s √† une VM, vous pouvez installer [votre propre instance LXD](https://canonical.com/lxd/install) sur une VM Linux dans votre ordinateur √† l'aide d'Oracle VirtualBox ou d'un autre logiciel similaire. Alternativement, si cette option ne fonctionne pas non plus pour vous, vous pouvez ex√©cuter les tests de charge directement dans votre ordinateur, sans utiliser une VM.
- **Cliquez sur l'onglet Advanced Options > Run time (temps d'ex√©cution)** : 120s (ou 2m)

> üìù **NOTE** : Les indicateurs mesur√©s par Locust correspondent aux [4 m√©triques d'or](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals) d√©finies par Google.

Lancez le test et observez les statistiques (onglet `Statistics`) et graphiques (onglet `Charts`) dans Locust. Au cours de ce test, vous allez observer que le nombre d'utilisateurs, les requ√™tes et le temps de r√©ponse augmentent progressivement. √Ä un certain point, le nombre d'√©checs (ligne rouge) devra commencer √† augmenter rapidement et ne jamais diminuer, indiquant que le Store Manager a atteint sa limite de capacit√© de fonctionnement normal (capture d'√©cran ci-dessous). Le moment pr√©cis de la panne peut varier en fonction des ressources de calcul disponibles sur votre machine virtuelle ou votre ordinateur.

![Graphique du test de charge](./docs/load-test.jpg)

> üí° **Question 1** : Combien d'utilisateurs faut-il pour que le Store Manager commence √† √©chouer dans votre environnement de test ? Pour r√©pondre √† cette question, comparez la ligne `Failures` et la ligne `Users` dans les graphiques.

> üí° **Question 2** : Sur l'onglet `Statistics`, comparez la diff√©rence entre les requ√™tes et les √©checs pour tous les endpoints. Combien d'entre eux √©chouent plus de 50 % du temps ?

> üí° **Question 3** : Affichez quelques exemples des messages d'erreur affich√©s dans l'onglet `Failures`. Ces messages indiquent une d√©faillance dans quelle(s) partie(s) du Store Manager ? Par exemple, est-ce que le probl√®me vient du service Python / MySQL / Redis / autre ?

Enregistrez le contenu du tableau `Statistics`, nous l'utiliserons plus tard pour comparer les tests suivants (par exemple, vous pouvez copier-coller le tableau dans Excel/Google Sheets ou dans un fichier texte).

### 6. Optimisez la lecture des donn√©es des articles
Nous avons v√©rifi√© que la performance du Store Manager ne r√©pond pas √† nos exigences (prise en charge de 150 utilisateurs simultan√©s). Avant d'envisager un changement d'architecture, de base de donn√©es, de serveur Web ou une augmentation des ressources (RAM/CPU) sur notre serveur on-premises ou en nuage, il est raisonnable de v√©rifier si une optimisation du code existant est possible. Cette approche pr√©sente g√©n√©ralement le meilleur rapport co√ªt-efficacit√©.

Dans `orders/commands/write_order.py`, si nous regardons attentivement la fonction `add_order`, nous verrons qu'elle ne r√©cup√®re pas les informations des articles de mani√®re efficace. Si nous avions, par exemple, 100 articles dans notre commande, la fonction effectuerait 100 requ√™tes √† la base de donn√©es pour chercher les informations sur les articles ([probl√®me N+1](https://planetscale.com/blog/what-is-n-1-query-problem-and-how-to-solve-it)).

```python
# ‚ùå Code non-optimis√©
product_prices = {}
for product_id in product_ids:
    product = session.query(Product).filter(Product.id == product_id).all()
    product_prices[product_id] = product[0].price
```

Pour r√©soudre ce probl√®me, modifiez la m√©thode `add_order` de fa√ßon √† collecter et r√©cup√©rer tous les `product_ids` en une seule requ√™te. Nous utiliserons toujours une boucle `for`, mais la requ√™te de base de donn√©es **ne se trouvera pas dans la boucle**.
```python
# ‚úÖ Code optimis√© (impl√©mentation partielle)
product_prices = {}
product_ids = [1, 2, 3] # TODO: Collectez le product_id de chaque OrderItem dans la commande
products = session.query(Product).filter(Product.id.in_(product_ids)).all()
for product in products:
    product_prices[product.id] = product.price
```

Red√©marrez votre conteneur `store_manager` pour vous assurer qu'aucun processus issu du test de charge pr√©c√©dent n'est en cours d'ex√©cution. 
```bash
docker compose restart store_manager                  
```

Ensuite, **relancez les tests Locust** avec les m√™mes param√®tres que ceux de la derni√®re activit√©. Observez et r√©pondez aux questions.

> üí° **Question 4** : Sur l'onglet `Statistics`, comparez les r√©sultats actuels avec les r√©sultats du test de charge pr√©c√©dent. Est-ce que vous voyez quelques diff√©rences dans les m√©triques pour l'endpoint `POST /orders` ?

> üí° **Question 5** : Si nous avions plus d'articles dans notre base de donn√©es (par exemple, 1 million), ou simplement plus d'articles par commande en moyenne, le temps de r√©ponse de l'endpoint `POST /orders` augmenterait-il, diminuerait-il ou resterait-il identique ?

Enregistrez le contenu du tableau `Statistics`, nous l'utiliserons plus tard pour comparer les tests suivants (par exemple, vous pouvez copier-coller le tableau dans Excel/Google Sheets ou dans un fichier texte).

> üìù **NOTE** : Bien que cela ne s'applique pas √† ce laboratoire, les applications complexes peuvent am√©liorer leurs performances de lecture en impl√©mentant plusieurs strat√©gies : la [cr√©ation d'index](https://www.w3schools.com/mysql/mysql_create_index.asp), la [normalisation des donn√©es](https://www.ibm.com/fr-fr/think/topics/database-normalization), ou l'augmentation du [nombre de connexions MySQL](https://dev.mysql.com/doc/refman/8.4/en/server-system-variables.html#sysvar_max_connections). Cependant, ces solutions ne constituent que des mesures temporaires si le v√©ritable enjeu r√©side dans la gestion d'un grand nombre de requ√™tes simultan√©es. Ce sujet est d√©taill√© dans la derni√®re section de ce document.

### 7. R√©activez Redis et optimisez la g√©n√©ration des rapports

√âtant donn√© que nous avons fait tout notre possible dans le code pour am√©liorer la vitesse d'√©criture, nous allons maintenant tenter d'am√©liorer la vitesse de lecture en utilisant le cache.Dans `queries/read_order.py`, remplacez l'appel √† `get_highest_spending_users_mysql` par `get_highest_spending_users_redis`. √âgalement, remplacez l'appel √† `get_best_selling_products_mysql` par `get_best_selling_products_redis`.

Red√©marrez votre conteneur `store_manager` pour vous assurer qu'aucun processus issu du test de charge pr√©c√©dent n'est en cours d'ex√©cution. Ensuite, **relancez les tests Locust** avec les m√™mes param√®tres que ceux de la derni√®re activit√©. Observez et enregistrez le contenu du tableau `Statistics`, nous l'utiliserons plus tard pour comparer les tests suivants (par exemple, vous pouvez copier-coller le tableau dans Excel/Google Sheets ou dans un fichier texte).

Contrairement √† ce que l'on pourrait croire, vous constaterez une augmentation g√©n√©rale du temps de r√©ponse et une diminution du nombre de requ√™tes trait√©es. Mais pourquoi ? M√™me si Redis est en m√©moire et que l'acc√®s √† la m√©moire est rapide, nous l'interrogeons tr√®s fr√©quemment pour obtenir la liste de commandes (`r.keys("order:*")`), puis nous parcourons cette liste, r√©cup√©rons l'objet commande (`r.hgetall(key)`) et le traitons pour g√©n√©rer le rapport. Cette approche prend trop de temps, et la dur√©e n√©cessaire augmente proportionnellement √† la quantit√© de commandes et d'articles par commande. Pour r√©soudre ce probl√®me, nous devons conserver le rapport en cache pendant une p√©riode d√©termin√©e. Le rapport ne sera d√©sormais plus mis √† jour en temps r√©el, mais cette solution nous permettra de servir des rapports tr√®s r√©cents de mani√®re quasi instantan√©e.

Veuillez copier et coller le code optimis√© fourni dans le r√©pertoire `/optimization`. Vous devez mettre √† jour l'impl√©mentation de `src/orders/queries/read_order.py` et ajouter l'extrait de code fourni au fichier `src/store_manager.py` apr√®s la d√©claration de la variable `app`. N'oubliez pas de mettre √† jour les appels aux rapports dans `src/orders/controllers/order_controller.py` en ajoutant le param√®tre `skip_cache` dans les 2 fonctions :

```py
def get_report_highest_spending_users(skip_cache=False):
    """Get orders report: highest spending users"""
    return get_highest_spending_users(skip_cache)

def get_report_best_selling_products(skip_cache=False):
    """Get orders report: best selling products"""
    return get_best_selling_products(skip_cache)
```

Le code optimis√© fera ce qui suit :
1. G√©n√©rer les 2 rapports et les enregistrer dans Redis d√®s que le Store Manager d√©marre. Dans ce cas, il ignorera simplement tout cache existant (`skip_cache=True`).
2. Les m√©thodes de g√©n√©ration de rapports ne liront que le rapport enti√®rement rendu d√©j√† mis en cache dans Redis (`skip_cache=False`), sans essayer de le r√©g√©n√©rer √† chaque fois ni de le r√©g√©n√©rer depuis MySQL.
3. Les rapports seront r√©g√©n√©r√©s toutes les 60 secondes afin d'afficher les derni√®res modifications. Peu importe le temps que prendra la g√©n√©ration, l'ancien rapport sera livr√© aux utilisateurs jusqu'√† ce que le nouveau soit disponible. Ainsi, le cache ne sera jamais vide et cela permet d'√©viter un probl√®me de [cache stampede](https://www.geeksforgeeks.org/system-design/cache-stempede-or-dogpile-problem-in-system-design/).

> üìù **NOTE** : Techniquement, r√©g√©n√©rer les rapports toutes les 60 secondes est un gaspillage de ressources, car nous les r√©g√©n√©rerons m√™me si personne ne les utilise ou m√™me lorsqu'il n'y a pas de changement. Cependant, il s'agit d'une optimisation simplifi√©e √† des fins didactiques pour ce laboratoire. Si vous souhaitez en savoir plus sur les solutions robustes et √©l√©gantes aux probl√®mes de ¬´ cache stampede ¬ª et ¬´ thundering herds ¬ª, veuillez lire cet article : [Thundering Herds: The Scalability Killer](https://docs.aonnis.com/blog/thundering-herds-the-scalability-killer).

Red√©marrez tous vos conteneurs (`docker compose restart`) pour vous assurer qu'aucun processus issu du test de charge pr√©c√©dent n'est en cours d'ex√©cution. Ensuite, **relancez les tests Locust** avec les m√™mes param√®tres que ceux de la derni√®re activit√©.

> üí° **Question 6** : Sur l'onglet `Statistics`, comparez les r√©sultats actuels avec les r√©sultats du test de charge pr√©c√©dent. Est-ce que vous voyez quelques diff√©rences significatives dans les m√©triques pour les endpoints `POST /orders`, `GET /orders/reports/highest-spenders` et `GET /orders/reports/best-sellers` ? Dans quelle mesure la performance s'est-elle am√©lior√©e ou d√©t√©rior√©e (par exemple, en pourcentage) ?

> üí° **Question 7** : La g√©n√©ration de rapports repose d√©sormais enti√®rement sur des requ√™tes adress√©es √† Redis, ce qui r√©duit la charge pesant sur MySQL. Cependant, le point de terminaison `POST /orders` reste √† la tra√Æne par rapport aux autres en termes de performances dans notre sc√©nario de test. Alors, qu'est-ce qui limite les performances de l'endpoint `POST /orders` ?

Encore une fois, enregistrez le contenu du tableau `Statistics`, nous l'utiliserons plus tard pour comparer les tests suivants.

### 8. Testez l'√©quilibrage de charge (load balancing) avec Nginx
C'est plus int√©ressant de tester l'√©quilibrage de charge en utilisant 2 VMs distantes, car cela nous donne acc√®s √† plus de ressources de calcul et ainsi √† une am√©lioration de performance plus significative. Cependant, si ce n'est pas possible pour vous, vous pouvez simplement cr√©er plusieurs instances de l'application Store Manager dans votre Docker pour observer comment fonctionne Nginx.

#### 8.1. Si vous utilisez des machines virtuelles (VMs) distantes
Tout d'abord, d√©ployez l'application Store Manager dans tous les VMs que vous voulez utiliser dans le test de charge. Le d√©ploiement peut √™tre automatique ou manuel. Appelez les endpoints sur chaque instance pour vous assurer de leur bon fonctionnement. Ensuite, dans votre ordinateur de d√©veloppement, utilisez les fichiers dans le r√©pertoire `load-balancer-config/scenario_81` :
- Copiez le texte dans `docker-compose-to-copy-paste.txt` et collez-le dans `docker-compose.yml`. Cela cr√©era un conteneur Nginx et 2 instances (replicas) √† Store Manager dans votre Docker.
- Cr√©ez un fichier `nginx.conf` dans le r√©pertoire racine du projet.
- Copiez le texte dans `nginx-conf-to-copy-paste.txt` et collez-le dans le fichier `nginx.conf`.
- Ajoutez les adresses de vos VMs cibles dans `nginx.conf`. Bien que nous vous recommandions d'utiliser 2 instances pour ce test, vous pourriez th√©oriquement en utiliser autant que vous le souhaitez.

#### 8.2. Si vous utilisez seulement votre ordinateur
Pour tester le sc√©nario suivant, utilisez le r√©pertoire `load-balancer-config/scenario_82` :
- Copiez le texte dans `docker-compose-to-copy-paste.txt` et collez-le dans `docker-compose.yml`. Cela cr√©era un conteneur Nginx dans votre Docker.
- Cr√©ez un fichier `nginx.conf` dans le r√©pertoire racine du projet.
- Copiez le texte dans `nginx-conf-to-copy-paste.txt` et collez-le dans le fichier `nginx.conf`.

Finalement, reconstruisez et red√©marrez vos conteneurs :
```sh
docker compose down
docker compose build
docker compose up -d
```

Attendez un peu que tous les conteneurs soient de nouveau op√©rationnels. Ensuite, **relancez les tests Locust** avec les m√™mes param√®tres que ceux de la derni√®re activit√©. Cependant, cette fois-ci, envoyez vos requ√™tes de Locust √† `nginx:80`. Enregistrez le contenu du tableau `Statistics`, nous l'utiliserons pour la comparaison finale.

> üí° **Question 8** : Sur l'onglet `Statistics`, comparez les r√©sultats actuels avec les r√©sultats du test de charge pr√©c√©dent. Est-ce que vous voyez quelques diff√©rences significatives dans les m√©triques pour les endpoints `POST /orders`, `GET /orders/reports/highest-spenders` et `GET /orders/reports/best-sellers` ? Dans quelle mesure la performance s'est-elle am√©lior√©e ou d√©t√©rior√©e (par exemple, en pourcentage) ? La r√©ponse d√©pendra de votre environnement d'ex√©cution (par exemple, vous obtiendrez de meilleures performances en ex√©cutant 2 instances de Store Manager sur 2 machines virtuelles plut√¥t que sur une seule).

> üí° **Question 9** : Dans le fichier `nginx.conf`, il existe un attribut qui configure l'√©quilibrage de charge. Quelle politique d'√©quilibrage de charge utilisons-nous actuellement ? Consultez la documentation officielle de Nginx si vous avez des questions.

### ‚≠ê Points cl√©s √† retenir de ce labo
L'objectif de ce laboratoire n'est pas de r√©soudre tous les probl√®mes de performance de l'application Store Manager, mais de la pousser √† ses limites afin que nous puissions observer comment elle r√©agit et l'optimiser en cons√©quence. Voici quelques points importants :

1. **La mesure est fondamentale**. C‚Äôest par la mesure que nous pouvons d√©terminer ce qu‚Äôil faut optimiser et si notre optimisation a r√©ussi. M√™me lorsque votre intuition vous dit que votre nouvelle impl√©mentation devrait apporter une am√©lioration (par exemple, le cache avec Redis), cela peut ne pas √™tre le cas, car elle entra√Æne une cons√©quence impr√©vue (surcharge des serveurs Python et Redis).
2. **Les m√©triques servent √† la prise de d√©cision**. Il est important non seulement de mesurer et observer, mais aussi d'agir en fonction de nos mesures, et d'√™tre en mesure de dire dans quelle mesure une impl√©mentation est meilleure ou pire par rapport √† une autre.
3. **Les effets d'une optimisation ne sont pas isol√©s**. Lorsque nous modifions l'impl√©mentation ou l'architecture de notre application pour am√©liorer une m√©trique, nous pouvons en d√©t√©riorer une autre.
4. Dans une application monolithique et synchrone telle que Store Manager, la **base de donn√©es devient rapidement une limitation**, en particulier si notre exigence est de prendre en charge un grand nombre d'utilisateurs simultan√©s. La solution consiste √† proc√©der √† une mise √† l'√©chelle verticale (par exemple, ajouter plus de RAM/CPU √† votre serveur), puis √† une mise √† l'√©chelle horizontale (√©quilibrage de charge), et enfin migrer vers une autre architecture (par exemple, les microservices event-driven). Nous aborderons certains de ces sujets dans les prochains laboratoires.

## üì¶ Livrables

- Un fichier .zip contenant l'int√©gralit√© du code source du projet Labo 04.
- Un rapport en .pdf r√©pondant aux questions pr√©sent√©es dans ce document. Il est obligatoire d'illustrer vos r√©ponses avec des extraits de code ou des captures d'√©cran/terminal.
